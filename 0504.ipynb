{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JVxQYY0zLjW"
      },
      "source": [
        "N-граммы - это языковые модели, которые используются для предсказания N-ного слова исходя из предыдущих N-1 слов."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCC4EDWKzLja"
      },
      "source": [
        "P(w<sub>i</sub> | w<sub>i-N+1</sub>, w<sub>i-N+2</sub>, ..., w<sub>i-1</sub>) = C(w<sub>i-N+1</sub>, w<sub>i-N+2</sub>, ..., w<sub>i-1</sub>, w<sub>i</sub>) / C(w<sub>i-N+1</sub>, w<sub>i-N+2</sub>, ..., w<sub>i-1</sub>)  (1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdLJvcv53LoJ"
      },
      "source": [
        "Словами: вероятность встретить слово w<sub>i</sub> после слов w<sub>i-N+1</sub>, w<sub>i-N+2</sub>, ..., w<sub>i-1</sub> (назовём это историей) равна отношению количества раз, которое всё сочетание слов вместе с w<sub>i</sub> втретилось в обучающей выборки, к количеству раз, которое в выборке встретилась история."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwM8N1CNzLja"
      },
      "source": [
        "Пример: биграммы (N = 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cf8TUiA4zLjb"
      },
      "outputs": [],
      "source": [
        "corpus = \"John read her book. I read a different book. John read a book by Mulan.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srNaOQUEzLjc"
      },
      "source": [
        "Чтобы P(w<sub>i</sub> | w<sub>i-1</sub>) имело смысл для i = 1, добавим в начало каждого предложения специальный токен &lt;s&gt;. В общем случае таких токенов должно быть N - 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQ84eqyRzLjc"
      },
      "source": [
        "Чтобы вероятности всех последовательностей в сумме составляли 1, добавим в конец каждого предложения специальный токен &lt;/s&gt;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGHLQK8IzLjd"
      },
      "source": [
        "Теперь посчитаем вероятность P(John read a book):\n",
        "\n",
        "P(John read a book) = P(John | &lt;s&gt;) * P(read | John) * P(a | read) * P(book | a) * P(&lt;/s&gt; | book)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jI0SeLevzLjd"
      },
      "source": [
        "А какова вероятность P(Mulan read a book)?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aheU9pazLje"
      },
      "source": [
        "Сглаживание Лапласа: будем делать вид, что все возможные биграммы встретились на один раз больше, чем в реальности. Для этого в формуле (1) прибавим в числителе 1, а в знаменателе - размер словаря (включая &lt;s&gt; и &lt;/s&gt;):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6qMMkPCzLje"
      },
      "source": [
        "P(w<sub>i</sub> | w<sub>i-N+1</sub>, w<sub>i-N+2</sub>, ..., \n",
        "w<sub>i-1</sub>) = (C(w<sub>i-N+1</sub>, w<sub>i-N+2</sub>, ..., w<sub>i-1</sub>, w<sub>i</sub>) + 1) / (C(w<sub>i-N+1</sub>, w<sub>i-N+2</sub>, ..., w<sub>i-1</sub>) + V) (2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SP9pyJsxzLjf"
      },
      "source": [
        "Обработка неизвестных слов: добавим в словарь токен &lt;unk&gt;, на который будем заменять все незнакомые слова (которые не входят в словарь). Благодаря сглаживанию Лапласа для всех N-грамм с этим словом будут ненулевые вероятности."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Вы также можете оценить вероятность &lt;unk&gt;, заменив на него редкие слова в обучающем корпусе."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kwdREbJzLjf"
      },
      "source": [
        "Задание: обучить две N-граммные модели на корпусе текстов студенческой конференции (две разные секции или группы секций - например, литературоведение и лингвистика). Взять текст, не входящий в обучающую выборку, и попробовать определить его вероятность по каждой из двух моделей. Где вероятность больше, той секции, скорее всего, он и будет принадлежать.\n",
        "\n",
        "1. Создать обучающие выборки\n",
        "\n",
        "1.1. Отобрать тексты для каждой секции\n",
        "\n",
        "1.2. Каждый текст разбить на предложения, очистить от пунктуации, привести к нижнему регистру\n",
        "\n",
        "1.3. Каждое предложение разбить на слова, добавить &lt;s&gt; и &lt;/s&gt;, добавить в список предложений\n",
        "\n",
        "1.4. Должно получиться два списка предложений (по одному на каждую секцию). Каждое предложение - это список слов, первое из которых - &lt;s&gt;, а последнее - &lt;/s&gt;. Следите, чтобы не было пустых предложений.\n",
        "\n",
        "2. Обучить на каждой выборке N-граммную модель (N = 3 или, в крайнем случае, 2; желательно иметь поддержку произвольного порядка N-грамм)\n",
        "\n",
        "2.1. Для каждой выборки составить лексикон (т.е. список встретившихся слов)\n",
        "\n",
        "2.2. В лексикон должны входить &lt;s&gt;, &lt;/s&gt; и &lt;unk&gt;\n",
        "\n",
        "2.3. Построить два словаря, где ключи - N-граммы и N-1-граммы, значения - их каунты. Эти словари и есть наша модель\n",
        "\n",
        "3. Использовать модель для определения принадлежности текста\n",
        "\n",
        "3.1. Взять текст, не входящий ни в одну из выборок (но желательно принадлежащий одной из двух секций)\n",
        "\n",
        "3.2. Очистить и разбить на предложения по алгоритму из п. 1.\n",
        "\n",
        "3.3. Для каждого предложения определить вероятность с помощью моделей по формуле (2) (не забывая заменять неизвестные модели слова на &lt;unk&gt; и применять сглаживание Лапласа)\n",
        "\n",
        "3.4. Определить вероятность текста как произведение вероятностей предложений\n",
        "\n",
        "3.5. Сравнить полученные вероятности и сделать выводы\n",
        "\n",
        "4. \\* Опциональное задание: написать программу генерации случайных текстов по вашим N-граммам (используя ```random.choices()``` с параметром ```weights```)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Замечание: будет удобнее, если вы будете хранить вероятности не в прямом представлении, а в логарифмическом (основание логарифма может быть любым). Это позволяет:\n",
        "1. Избежать underflow\n",
        "2. Заменить умножение на сложение"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!wget https://pkholyavin.github.io/year4programming/conference_stud_2015-2023.pkl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FMLmlgq3LoP"
      },
      "source": [
        "Модуль pickle для сериализации (хранения) данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ethAXkxx5weW"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "with open(\"conference_stud_2015-2023.pkl\", \"rb\") as f:\n",
        "    data = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aCfXNWRn6dEk"
      },
      "outputs": [],
      "source": [
        "with open(\"new_pickle.pkl\", \"wb\") as f:\n",
        "    pickle.dump(data, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfqkDcgr3LoS"
      },
      "source": [
        "Посмотрим на список секций"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9U7VzFh68D7",
        "outputId": "e7c1f87d-4446-4928-ad59-82d4a269965c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Антрополингвистика: Человек, Язык, Культура',\n",
              " 'Балканистика. Византинистика. Неоэллинистика',\n",
              " 'Библеистика',\n",
              " 'Будетляне. Гипотеза в филологии: научные бои',\n",
              " 'Грамматика (романо-германская филология)',\n",
              " 'Грамматика и семантика русского языка',\n",
              " 'Дискурс и текст',\n",
              " 'История зарубежных литератур',\n",
              " 'История и диалектология русского языка',\n",
              " 'История и теория русского и западноевропейского стиха',\n",
              " 'История русского языка',\n",
              " 'Кино|Текст',\n",
              " 'Киноперевод',\n",
              " 'Классическая филология',\n",
              " 'Коллоквиалистика (анализ устной речи)',\n",
              " 'Компьютерная и прикладная лингвистика',\n",
              " 'Лексикология (романо-германская филология)',\n",
              " 'Лексикология и стилистика русского языка',\n",
              " 'Лингвометодические основы описания и изучения русского языка как иностранного',\n",
              " 'Литература в междисциплинарной перспективе',\n",
              " 'Мастерская анализа кинотекста',\n",
              " 'Междисциплинарная постерная сессия',\n",
              " 'Общее языкознание',\n",
              " 'Переводоведение (романо-германская филология)',\n",
              " 'Пленарное заседание',\n",
              " 'Прикладная и математическая лингвистика',\n",
              " 'Психолингвистика',\n",
              " 'Русская литература: история',\n",
              " 'Русская литература: теория, поэтика',\n",
              " 'Славяно-германская компаративистика',\n",
              " 'Славянская филология: языкознание и литературоведение',\n",
              " 'Театр',\n",
              " 'Финно-угорская филология',\n",
              " 'Фольклор и мифология',\n",
              " 'Фонетика'}"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "set([i[\"section\"] for i in data])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
