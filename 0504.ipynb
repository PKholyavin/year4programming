{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-граммы - это языковые модели, которые используются для предсказания N-ного слова исходя из предыдущих N-1 слов."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P(w<sub>i</sub> | w<sub>i-N+1</sub>, w<sub>i-N+2</sub>, ..., w<sub>i-1</sub>) = C(w<sub>i-N+1</sub>, w<sub>i-N+2</sub>, ..., w<sub>i-1</sub>, w<sub>i</sub>) / C(w<sub>i-N+1</sub>, w<sub>i-N+2</sub>, ..., w<sub>i-1</sub>)  (1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример: биграммы (N = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"John read her book. I read a different book. John read a book by Mulan.\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы P(w<sub>i</sub> | w<sub>i-1</sub>) имело смысл для i = 1, добавим в начало каждого предложения специальный токен &lt;s&gt;. В общем случае таких токенов должно быть N - 1."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы вероятности всех последовательностей в сумме составляли 1, добавим в конец каждого предложения специальный токен &lt;/s&gt;"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь посчитаем вероятность P(John read a book):\n",
    "\n",
    "P(John read a book) = P(John | &lt;s&gt;) * P(read | John) * P(a | read) * P(book | a) * P(&lt;/s&gt; | book)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А какова вероятность P(Mulan read a book)?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сглаживание Лапласа: будем делать вид, что все возможные биграммы встретились на один раз больше, чем в реальности. Для этого в формуле (1) прибавим в числителе 1, а в знаменателе - размер словаря (включая &lt;s&gt; и &lt;/s&gt;):"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P(w<sub>i</sub> | w<sub>i-N+1</sub>, w<sub>i-N+2</sub>, ..., w<sub>i-1</sub>) = (C(w<sub>i-N+1</sub>, w<sub>i-N+2</sub>, ..., w<sub>i-1</sub>, w<sub>i</sub>) + 1) / (C(w<sub>i-N+1</sub>, w<sub>i-N+2</sub>, ..., w<sub>i-1</sub>) + V) (2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обработка неизвестных слов: добавим в словарь токен &lt;unk&gt;, на который будем заменять все незнакомые слова (которые не входят в словарь). Благодаря сглаживанию Лапласа для всех N-грамм с этим словом будут ненулевые вероятности."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание: обучить две N-граммные модели на корпусе текстов студенческой конференции (две разные секции). Взять текст, не входящий в обучающую выборку, и попробовать определить его вероятность по каждой из двух моделей. Где вероятность больше, той секции, скорее всего, он и будет принадлежать."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
